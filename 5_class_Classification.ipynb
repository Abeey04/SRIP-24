{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461696ab-a33c-4acf-b947-f70202b6a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display\n",
    "import os\n",
    "import cv2\n",
    "from os import listdir\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils import img_to_array, load_img\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc \n",
    "\n",
    "import matplotlib\n",
    "import gc\n",
    "matplotlib.use('Agg') # No pictures displayed \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16516b42-6cde-4644-8327-8aac7435812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"C:\\Users\\rudra\\OneDrive\\Desktop\\animals\\5_Class\"\n",
    "save_path = r\"C:\\Users\\rudra\\OneDrive\\Desktop\\animals\\5_Class\"\n",
    "classes = listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f3163b-1f34-42c8-9a16-20f9a3ce6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list, label_list = [] , []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4543ac-f405-48a6-a103-64cf19e79391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading images ...\n",
      "[INFO] Processing Aerial ...\n",
      "[INFO] Processing Aquatic ...\n",
      "[INFO] Processing Arboreal ...\n",
      "[INFO] Processing Burrowing ...\n",
      "[INFO] Processing Terrestrial ...\n",
      "[INFO] image extract completed\n",
      "total train samples: \n",
      "5399\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        print(\"[INFO] Loading images ...\")\n",
    "        image_folder_list = listdir(train_path)\n",
    "        for image_folder in image_folder_list:\n",
    "            print(f\"[INFO] Processing {image_folder} ...\")\n",
    "            images_list = listdir(f\"{train_path}/{image_folder}/\")\n",
    "            for samples in images_list[:]:\n",
    "                image_directory = f\"{train_path}/{image_folder}/{samples}\"\n",
    "                if image_directory.endswith(\".jpg\")==True or image_directory.endswith(\".JPG\")==True or image_directory.endswith(\".jpeg\")==True:  \n",
    "                    image_list.append(img_to_array(load_img(image_directory,target_size=(128,128))))\n",
    "                    label_list.append(image_folder)\n",
    "    \n",
    "        print(\"[INFO] image extract completed\")  \n",
    "        print(\"total train samples: \")\n",
    "        print(len(image_list))\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8681c0ba-8e31-4861-bc50-05e15b75ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert features (X) and labels (y) to Numpy arrays\n",
    "X = np.array(image_list)/255\n",
    "y = np.array(label_list)\n",
    "image_list, label_list = [] , []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1e05c5-f386-4c90-a1b4-ed57ccf86885",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "Y = to_categorical(le.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb646eab-c0d3-452a-9772-110e7fe63c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes:  5\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(le.classes_)\n",
    "print(\"Total number of classes: \", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487c42da-b0bd-459e-925b-6e66cf6ee484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aerial', 'Aquatic', 'Arboreal', 'Burrowing', 'Terrestrial']\n"
     ]
    }
   ],
   "source": [
    "labels = classes\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c557ca6-96bd-4633-a693-b42c34c79230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    num_rows = 128\n",
    "    num_columns = 128\n",
    "    num_channels = 3\n",
    "    inputShape = (num_rows, num_columns, num_channels)\n",
    "    chanDim = -1\n",
    "    LR = 1e-3\n",
    "    BATCH_SIZE = 32\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        inputShape = (num_rows, num_columns, num_channels)\n",
    "        chanDim = 1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=inputShape))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab2d63a-a0ef-4475-8135-983f2e9ac29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "def compute_confusion_matrix(y_true, y_pred,normalize=False):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1414168-dd09-41f0-9980-3eb93c48440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          classes, \n",
    "                          normalized=False, \n",
    "                          title=None, \n",
    "                          cmap=plt.cm.Blues,\n",
    "                          size=(10,10)):\n",
    "    fig, ax = plt.subplots(figsize=size)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalized else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8acdc265-833a-4f13-89b9-8255d225a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layers(model, img):\n",
    "    layer_outputs = [layer.output for layer in model.layers if isinstance(layer, Conv2D)]\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "    activations = activation_model.predict(np.expand_dims(img, axis=0))\n",
    "    layer = 1\n",
    "    for layer_activation in activations:\n",
    "        n_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // 16\n",
    "        display_grid = np.zeros((size * n_cols, 16 * size))\n",
    "        for col in range(n_cols):\n",
    "            for row in range(16):\n",
    "                channel_image = layer_activation[0, :, :, col * 16 + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "        plt.title(f'Conv Layer {layer}')\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "        layer = layer + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6365425d-e1c0-41ba-9bb5-9f176dfe2cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training network fold 1...\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 15s 52ms/step - loss: 1.5073 - accuracy: 0.3758\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 1.3829 - accuracy: 0.4396\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 1.3698 - accuracy: 0.4320\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 1.2247 - accuracy: 0.5102\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 1.1473 - accuracy: 0.5467\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 1.0492 - accuracy: 0.5879\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 6s 50ms/step - loss: 0.8668 - accuracy: 0.6751\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.7062 - accuracy: 0.7438\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.5795 - accuracy: 0.7939\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.3321 - accuracy: 0.8907\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.2157 - accuracy: 0.9372\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.1291 - accuracy: 0.9683\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.1007 - accuracy: 0.9736\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0735 - accuracy: 0.9793\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0706 - accuracy: 0.9818\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 6s 51ms/step - loss: 0.0569 - accuracy: 0.9851\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0573 - accuracy: 0.9851\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0703 - accuracy: 0.9793\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0503 - accuracy: 0.9857\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.0631 - accuracy: 0.9793\n",
      "[INFO] Saving Model fold 1 ...\n",
      "57/57 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rudra\\AppData\\Local\\Temp\\ipykernel_24632\\3249705492.py:34: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training network fold 2...\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 7s 50ms/step - loss: 1.5192 - accuracy: 0.3691\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 6s 50ms/step - loss: 1.3344 - accuracy: 0.4623\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 1.2087 - accuracy: 0.5111\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 1.0506 - accuracy: 0.5868\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.8809 - accuracy: 0.6678\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.6971 - accuracy: 0.7365\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.5143 - accuracy: 0.8175\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.3731 - accuracy: 0.8738\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.2778 - accuracy: 0.9111\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 6s 50ms/step - loss: 0.1866 - accuracy: 0.9406\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.1602 - accuracy: 0.9523\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 0.1281 - accuracy: 0.9565\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.1022 - accuracy: 0.9700\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 0.1044 - accuracy: 0.9650\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.0864 - accuracy: 0.9728\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 0.0707 - accuracy: 0.9773\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.0882 - accuracy: 0.9717\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.0650 - accuracy: 0.9779\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0546 - accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.0796 - accuracy: 0.9750\n",
      "[INFO] Saving Model fold 2 ...\n",
      "57/57 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rudra\\AppData\\Local\\Temp\\ipykernel_24632\\3249705492.py:34: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training network fold 3...\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 7s 50ms/step - loss: 1.4562 - accuracy: 0.3951\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 6s 51ms/step - loss: 1.2847 - accuracy: 0.4773\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 6s 52ms/step - loss: 1.1935 - accuracy: 0.5305\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 6s 50ms/step - loss: 1.3114 - accuracy: 0.4664\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 6s 50ms/step - loss: 1.1030 - accuracy: 0.5605\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 6s 50ms/step - loss: 0.9642 - accuracy: 0.6326\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.7617 - accuracy: 0.7195\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.5917 - accuracy: 0.7923\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.3798 - accuracy: 0.8730\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.2895 - accuracy: 0.9061\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.2222 - accuracy: 0.9299\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.1975 - accuracy: 0.9361\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.1288 - accuracy: 0.9596\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0852 - accuracy: 0.9762\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.0730 - accuracy: 0.9818\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 0.0553 - accuracy: 0.9865\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0449 - accuracy: 0.9910\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 6s 50ms/step - loss: 0.0405 - accuracy: 0.9879\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 6s 49ms/step - loss: 0.0398 - accuracy: 0.9882\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 6s 50ms/step - loss: 0.0454 - accuracy: 0.9863\n",
      "[INFO] Saving Model fold 3 ...\n",
      "57/57 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rudra\\AppData\\Local\\Temp\\ipykernel_24632\\3249705492.py:34: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits = 3, shuffle=True, random_state=42)\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "EPOCHS =20\n",
    "BATCH_SIZE = 32\n",
    "gc.collect()\n",
    "for train,test in cv.split(X,Y):\n",
    "    model = create_model()\n",
    "    print(f\"[INFO] Training network fold {fold_no}...\")\n",
    "    history = model.fit(X[train],Y[train], batch_size=BATCH_SIZE,\n",
    "                                  steps_per_epoch=len(X[train]) // BATCH_SIZE,\n",
    "                                  epochs=EPOCHS, \n",
    "                                  verbose=1)\n",
    "    gc.collect()\n",
    "    print(f\"[INFO] Saving Model fold {fold_no} ...\")\n",
    "    model.save(f\"{save_path}/model_5_class_fold\"+str(fold_no)+\".h5\")\n",
    "    scores = model.evaluate(X[test],Y[test],verbose=0)\n",
    "    acc_per_fold.append(scores[1]*100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    y_probs = model.predict(X[test])\n",
    "    # Get predicted labels\n",
    "    # yhat_probs = list(map(lambda x: 0 if (x<0.5) else 1,y_probs))\n",
    "    yhat_probs = np.argmax(y_probs, axis=1)\n",
    "    y_trues = y_trues = np.argmax(Y[test], axis=1)\n",
    "    cm = confusion_matrix(y_trues, yhat_probs)\n",
    "    plt.axis('off') # no axis\n",
    "    # pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n",
    "    plot_confusion_matrix(cm,\n",
    "                              labels, \n",
    "                              normalized=False, \n",
    "                              title=f\"Model Performance of 5_class fold: {fold_no}\", \n",
    "                              cmap=plt.cm.Blues,\n",
    "                              size=(12,12))\n",
    "    plt.savefig(f'{save_path}/Model Performance of 5_class fold {fold_no}.jpg', bbox_inches=None, pad_inches=0)\n",
    "    matplotlib.pyplot.close()\n",
    "    gc.collect()\n",
    "    fold_no = fold_no+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297b5afb-de31-48cb-8b45-8d14bd08df8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58.83333086967468, 56.166666746139526, 59.75542068481445]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a817f3f-57c0-4a5f-89de-9f74256920e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.9911105632781982, 1.8193013668060303, 2.0265347957611084]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "570367f8-02e4-4cab-8ad8-4b0df62a381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 144ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rudra\\AppData\\Local\\Temp\\ipykernel_24632\\1767733992.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  channel_image /= channel_image.std()\n",
      "C:\\Users\\rudra\\AppData\\Local\\Temp\\ipykernel_24632\\1767733992.py:18: RuntimeWarning: invalid value encountered in cast\n",
      "  channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n"
     ]
    }
   ],
   "source": [
    "visualize_conv_layers(model,img_to_array(load_img(r\"C:\\Users\\rudra\\OneDrive\\Desktop\\animals\\OvR3\\antelope\\rest\\0a1f4371e3.jpg\",target_size=(128,128)))/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06632961-2cda-4627-bc69-0f1839fa76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(f'{save_path}/Conv Visualization of 5_class.jpg', bbox_inches=None, pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a65f5a-cdcf-4ddd-ab4c-0b03dcecfb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
